---
title: "P8105 HW6"
author: Shihui Zhu sz3029
output: github_document
---

```{r setup, include=FALSE}
# Reproducibility
set.seed(1)

# This chunk loads all the packages used in this homework
library(tidyverse)
library(viridis)
library(ggridges)
library(patchwork)
library(leaps)

# General figure set up
knitr::opts_chunk$set(
  # display the code in github doc
  echo = TRUE,
  # hide warning messages
  warning = FALSE,
  message = FALSE,
  # set the figure to be 8 x 6, and the proportion it takes to be 90%
  fig.width = 8,
  fig.height = 6, 
  out.width = "90%"
)

# setting a global options for continuous data color family and a different format to set discrete data to have a color family
options(
  ggplot2.countinuous.colour = "viridis",
  ggplot2.countinuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

# have a minimal theme and legends at the bottom
theme_set(theme_minimal() + theme(legend.position = "bottom"))
```

## Problem 1

### Import data

Load and clean the data for regression analysis
```{r import_data, message=FALSE}
birthweight_df <- 
  read_csv("data/birthweight.csv") %>%
  drop_na() %>%
  mutate(
    # change the numbers to text for a better readability
    babysex = ifelse(babysex == 1, 'male', 'female'),
    # convert to factor
    babysex = fct_infreq(babysex),
    # change the numbers to text for a better readability
    frace = case_when(
      frace == 1 ~ 'white',
      frace == 2 ~ 'black',
      frace == 3 ~ 'asian',
      frace == 4 ~ 'puerto_rican',
      frace == 8 ~ 'other',
      frace == 9 ~ 'unknown'),
    # convert to factor
    frace = fct_infreq(frace),
    # change the numbers to text for a better readability
    malform = ifelse(malform == 1, 'present', 'absent'),
    # convert to factor
    malform = fct_infreq(malform),
    # change the numbers to text for a better readability
    mrace = case_when(
      mrace == 1 ~ 'white',
      mrace == 2 ~ 'black',
      mrace == 3 ~ 'asian',
      mrace == 4 ~ 'puerto_rican',
      mrace == 8 ~ 'other'),
    # convert to factor
    mrace = fct_infreq(mrace),
    )

birthweight_df
```

### Build model

They are 19 variables in total. We want to select the most important variables that affect baby's birth weight to build the regression model. We will use a data-driven model building process, backwards elimination.

1. Select variables using backwards elimination, using AIC criteria
```{r mlr}
mult.fit = lm(bwt ~ ., data = birthweight_df)
model.best = step(mult.fit, direction = 'backward')
```

The predictors of the "best" model proposed by backward selection is includes `babysex`, `bhead`, `blength`, `delwt`, `fincome`, `gaweeks`, `mheight`, `mrace`, `parity`, `ppwt`, and `smoken`:

```{r best}
model.best$coefficients %>% knitr::kable(digits = 3, col.names = c("Coefficient"))
```

2. Select the variables and build the MLR model

```{r model}
model_1 = lm(bwt ~ babysex + bhead + blength + delwt + gaweeks + fincome + 
               mheight + mrace + parity +
               ppwt + smoken, data = birthweight_df)

model_1 %>%
  broom::tidy() %>% 
  select(term, estimate, p.value) %>% 
  mutate(term = str_replace(term, "^babysex", "Baby sex: "),
         term = str_replace(term, "^mrace", "Mother race: ")) %>% 
  knitr::kable(digits = 3)
```

### Diagnostics

```{r resid}
# Residuals
modelr::add_residuals(birthweight_df, model_1) %>% head(10) %>% knitr::kable(digits = 3)

# Predictions
modelr::add_predictions(birthweight_df, model_1) %>% head(10) %>% knitr::kable(digits = 3)
```

Show an example model residuals against fitted values

```{r plot_ex, message=FALSE}
birthweight_df %>% 
  modelr::add_residuals(model_1) %>% 
  modelr::add_predictions(model_1) %>% 
  ggplot(aes(x = pred, y = resid)) + 
  geom_point() +
  geom_smooth(method = "lm", se = TRUE) +
  labs(
    title = "Residuals v.s. Fitted Values of Birth Weight",
    x = 'Fitted Values',
    y = 'Residuals'
  )
```

The residuals follows a normal distribution of a mean of 0. 

## Compare the model to two others

Use `cross_mc` to prepare CV training and testing sets
```{r}
#Splitting
cv_df = 
  modelr::crossv_mc(birthweight_df, length(birthweight_df)) 
#training
cv_df %>% pull(train) %>% .[[1]] %>% as_tibble
#testing
cv_df %>% pull(test) %>% .[[1]] %>% as_tibble

```

Compare the above model to two others

* One using length at birth and gestational age as predictors (main effects only)

* One using head circumference, length, sex, and all interactions (including the three-way interaction) between these
```{r}
cv_df =
  cv_df %>% 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble))

model_1 = lm(bwt ~ babysex + bhead + blength + frace + gaweeks + 
               mheight + momage + mrace +
               ppwt + smoken + wtgain, data = birthweight_df)

cv_df = 
  cv_df %>% 
  mutate(
    mod_1  = map(train, ~lm(bwt ~ babysex + bhead + blength + frace + gaweeks + 
               mheight + momage + mrace +
               ppwt + smoken + wtgain, data = .x)),
    mod_2  = map(train, ~lm(bwt ~ blength + gaweeks, data = .x)),
    mod_3  = map(train, ~lm(bwt ~ babysex + bhead + blength + babysex * bhead * blength, data = .x))) %>% 
  mutate(
    rmse_mod1 = map2_dbl(mod_1, test, ~modelr::rmse(model = .x, data = .y)),
    rmse_mod2 = map2_dbl(mod_2, test, ~modelr::rmse(model = .x, data = .y)),
    rmse_mod3 = map2_dbl(mod_3, test, ~modelr::rmse(model = .x, data = .y)))

cv_df %>% select(.id, rmse_mod1:rmse_mod3) %>% knitr::kable(digits = 3, 
                                                            caption = "RMSE Table for the 3 models",
                                                            col.names = c("Sample ID", "RMSE of Model 1", "RMSE of Model 2", "RMSE of Model 3"))
```

From the chart above, we see that the RMSE of model 1 < RMSE of model 3 < RMSE of model 2. The violin plots below also show the same pattern:

```{r}
cv_df %>% 
  select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") %>% 
  mutate(model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse)) + geom_violin() +
  labs(
    title = "RMSE for each models",
    x = 'Model',
    y = 'RMSE'
  )
```


## Problem 2

### Load the weather dataset

```{r p2, message=FALSE}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())

weather_df
```

### Parameter Inference via Bootstrapping

We’ll focus on a simple linear regression with `tmax` as the response and `tmin` as the predictor, and are interested in the distribution of two quantities estimated from 
these data:

* $\hat{r}^2$

* log(beta_0 * beta_1)

1. Construct 5000 bootstrap samples, and produce estimates of these two quantities

```{r}
boot_straps <-
  weather_df %>% 
  modelr::bootstrap(n = 5000) %>% 
  mutate(
    models = map(strap, ~lm(tmax ~ tmin, data = .x) ),
    results = map(models, broom::tidy),
    summary = map(models, summary),
    r_squared = map_dbl(summary, "r.squared")) %>% 
  select(-strap, -models, -summary) %>% 
  unnest(results) %>% 
  select(.id, term, r_squared, estimate) %>%
  pivot_wider(
    names_from = "term",
    values_from = "estimate",
    names_prefix = "estimate_"
  ) %>% 
  janitor::clean_names() %>%
  mutate(
    log_beta_0_beta_1 = log(estimate_intercept*estimate_tmin)
  ) %>%
  select(id, r_squared, log_beta_0_beta_1)
  
boot_straps
```

2. Plot the distribution of the estimates 

```{r}
# for r-squared estimate
boot_straps %>%
  ggplot(aes(x = r_squared)) +
  geom_density() +
  labs(
    title = "Density Plot for R-Square Estimation",
    x = 'R-Square Estimates',
    y = 'Density'
  )

boot_straps %>%
  ggplot(aes(x = log_beta_0_beta_1)) +
  geom_density() +
  labs(
    title = "Density Plot for log(beta_0 * beta_1) Estimation",
    x = 'log(beta_0 * beta_1) Estimates',
    y = 'Density'
  )
```

**Description**

For $\hat{r}^2$ :

This distribution is a little bit left-skewed with a mean of 0.91, with a bit of a “shoulder” on the left end, features that may be related to the frequency with which some small outliers are included in the bootstrap sample.

For log(beta_0 * beta_1):

This distribution has a normal distribution shape around mean of 2.02, with a bit of a “shoulder” on the right end, features that may be related to the frequency with which some large outliers are included in the bootstrap sample.


3. Identify the $2.5\%$ and $97.5\%$ quantiles to provide a $95\%$ confidence interval

```{r ci}
boot_straps %>%
  summarize(
    ci_lower_r_sq = quantile(r_squared, 0.025), 
    ci_upper_r_sq = quantile(r_squared, 0.975),
    ci_lower_log = quantile(log_beta_0_beta_1, 0.025), 
    ci_upper_log = quantile(log_beta_0_beta_1, 0.975)) %>%
  knitr::kable(digits = 3, col.names = c("95% CI lower bound for R squared est.",
                                         "95% CI upper bound for R squared est.",
                                         "95% CI lower bound for log(beta_0 * beta_1) est.",
                                         "95% CI upper bound for log(beta_0 * beta_1) est."))
```

The 95% confidence interval for $\hat{r}^2$ is $(0.894, 0.928)$, and the 95% confidence interval for log(beta_0 * beta_1) is $(1.965, 2.059)$.
